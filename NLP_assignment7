{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPfmBqUbAqQTeSN6imctOH/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harshavardhanrajuthonti/NLP/blob/main/NLP_assignment7\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Example dataset (small English to French pairs)\n",
        "data = [(\"hello\", \"bonjour\"),\n",
        "        (\"how are you\", \"comment ça va\"),\n",
        "        (\"I am fine\", \"je vais bien\"),\n",
        "        (\"what is your name\", \"comment tu t'appelles\"),\n",
        "        (\"my name is\", \"je m'appelle\"),\n",
        "        (\"thank you\", \"merci\"),\n",
        "        (\"goodbye\", \"au revoir\")]\n",
        "\n",
        "# Separate into English and French sentences\n",
        "english_sentences = [pair[0] for pair in data]\n",
        "french_sentences = [pair[1] for pair in data]\n",
        "\n",
        "# Tokenize the English and French sentences\n",
        "english_tokenizer = Tokenizer()\n",
        "french_tokenizer = Tokenizer()\n",
        "\n",
        "english_tokenizer.fit_on_texts(english_sentences)\n",
        "french_tokenizer.fit_on_texts(french_sentences)\n",
        "\n",
        "# Convert text to sequences of integers\n",
        "english_sequences = english_tokenizer.texts_to_sequences(english_sentences)\n",
        "french_sequences = french_tokenizer.texts_to_sequences(french_sentences)\n",
        "\n",
        "# Pad the sequences to have uniform length\n",
        "max_english_seq_len = max([len(seq) for seq in english_sequences])\n",
        "max_french_seq_len = max([len(seq) for seq in french_sequences])\n",
        "\n",
        "english_padded = pad_sequences(english_sequences, maxlen=max_english_seq_len, padding='post')\n",
        "french_padded = pad_sequences(french_sequences, maxlen=max_french_seq_len, padding='post')\n",
        "\n",
        "# Get the size of the vocabularies\n",
        "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
        "french_vocab_size = len(french_tokenizer.word_index) + 1\n",
        "\n",
        "# Output\n",
        "print(\"English Sentences (Original Text):\")\n",
        "print(english_sentences)\n",
        "\n",
        "print(\"\\nFrench Sentences (Original Text):\")\n",
        "print(french_sentences)\n",
        "\n",
        "print(\"\\nTokenized English Sequences:\")\n",
        "print(english_sequences)\n",
        "\n",
        "print(\"\\nTokenized French Sequences:\")\n",
        "print(french_sequences)\n",
        "\n",
        "print(\"\\nPadded English Sequences:\")\n",
        "print(english_padded)\n",
        "\n",
        "print(\"\\nPadded French Sequences:\")\n",
        "print(french_padded)\n",
        "\n",
        "print(f\"\\nEnglish Vocabulary Size: {english_vocab_size}\")\n",
        "print(f\"French Vocabulary Size: {french_vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRf_bWX3gv8l",
        "outputId": "39653651-24be-4406-a34a-076806dfdc96"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Sentences (Original Text):\n",
            "['hello', 'how are you', 'I am fine', 'what is your name', 'my name is', 'thank you', 'goodbye']\n",
            "\n",
            "French Sentences (Original Text):\n",
            "['bonjour', 'comment ça va', 'je vais bien', \"comment tu t'appelles\", \"je m'appelle\", 'merci', 'au revoir']\n",
            "\n",
            "Tokenized English Sequences:\n",
            "[[4], [5, 6, 1], [7, 8, 9], [10, 2, 11, 3], [12, 3, 2], [13, 1], [14]]\n",
            "\n",
            "Tokenized French Sequences:\n",
            "[[3], [1, 4, 5], [2, 6, 7], [1, 8, 9], [2, 10], [11], [12, 13]]\n",
            "\n",
            "Padded English Sequences:\n",
            "[[ 4  0  0  0]\n",
            " [ 5  6  1  0]\n",
            " [ 7  8  9  0]\n",
            " [10  2 11  3]\n",
            " [12  3  2  0]\n",
            " [13  1  0  0]\n",
            " [14  0  0  0]]\n",
            "\n",
            "Padded French Sequences:\n",
            "[[ 3  0  0]\n",
            " [ 1  4  5]\n",
            " [ 2  6  7]\n",
            " [ 1  8  9]\n",
            " [ 2 10  0]\n",
            " [11  0  0]\n",
            " [12 13  0]]\n",
            "\n",
            "English Vocabulary Size: 15\n",
            "French Vocabulary Size: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
        "\n",
        "# Define model parameters\n",
        "embedding_dim = 64\n",
        "lstm_units = 128\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_english_seq_len,))\n",
        "encoder_embedding = Embedding(english_vocab_size, embedding_dim)(encoder_inputs)\n",
        "encoder_lstm, encoder_state_h, encoder_state_c = LSTM(lstm_units, return_state=True)(encoder_embedding)\n",
        "encoder_states = [encoder_state_h, encoder_state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(max_french_seq_len,))\n",
        "decoder_embedding = Embedding(french_vocab_size, embedding_dim)(decoder_inputs)\n",
        "decoder_lstm, _, _ = LSTM(lstm_units, return_sequences=True, return_state=True)(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(french_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_lstm)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "drnQF0-Fg0Fh",
        "outputId": "dedeffc8-9f86-49dc-a958-5aa9c1a28ea6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_21            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_22            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │            \u001b[38;5;34m960\u001b[0m │ input_layer_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │            \u001b[38;5;34m896\u001b[0m │ input_layer_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_13 (\u001b[38;5;33mLSTM\u001b[0m)            │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   │         \u001b[38;5;34m98,816\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│                           │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]     │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_14 (\u001b[38;5;33mLSTM\u001b[0m)            │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m),       │         \u001b[38;5;34m98,816\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,    │                │ lstm_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],         │\n",
              "│                           │ \u001b[38;5;34m128\u001b[0m)]                  │                │ lstm_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m14\u001b[0m)          │          \u001b[38;5;34m1,806\u001b[0m │ lstm_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_21            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_22            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │            <span style=\"color: #00af00; text-decoration-color: #00af00\">960</span> │ input_layer_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │            <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │ input_layer_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)            │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]     │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)            │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,    │                │ lstm_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],         │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]                  │                │ lstm_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,806</span> │ lstm_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m201,294\u001b[0m (786.30 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">201,294</span> (786.30 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m201,294\u001b[0m (786.30 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">201,294</span> (786.30 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Example dataset (small English to French pairs)\n",
        "data = [(\"hello\", \"bonjour\"),\n",
        "        (\"how are you\", \"comment ça va\"),\n",
        "        (\"I am fine\", \"je vais bien\"),\n",
        "        (\"what is your name\", \"comment tu t'appelles\"),\n",
        "        (\"my name is\", \"je m'appelle\"),\n",
        "        (\"thank you\", \"merci\"),\n",
        "        (\"goodbye\", \"au revoir\")]\n",
        "\n",
        "# Separate into English and French sentences\n",
        "english_sentences = [pair[0] for pair in data]\n",
        "french_sentences = [pair[1] for pair in data]\n",
        "\n",
        "# Tokenize the English and French sentences\n",
        "english_tokenizer = Tokenizer()\n",
        "french_tokenizer = Tokenizer()\n",
        "\n",
        "english_tokenizer.fit_on_texts(english_sentences)\n",
        "french_tokenizer.fit_on_texts(french_sentences)\n",
        "\n",
        "# Convert text to sequences of integers\n",
        "english_sequences = english_tokenizer.texts_to_sequences(english_sentences)\n",
        "french_sequences = french_tokenizer.texts_to_sequences(french_sentences)\n",
        "\n",
        "# Pad the sequences to have uniform length\n",
        "max_english_seq_len = max([len(seq) for seq in english_sequences])\n",
        "max_french_seq_len = max([len(seq) for seq in french_sequences])\n",
        "\n",
        "english_padded = pad_sequences(english_sequences, maxlen=max_english_seq_len, padding='post')\n",
        "french_padded = pad_sequences(french_sequences, maxlen=max_french_seq_len, padding='post')\n",
        "\n",
        "# Prepare decoder input and output data\n",
        "decoder_input_data = french_padded[:, :-1]\n",
        "decoder_output_data = french_padded[:, 1:]\n",
        "\n",
        "# Expand decoder output to 3D shape for sparse_categorical_crossentropy\n",
        "decoder_output_data = np.expand_dims(decoder_output_data, -1)\n",
        "\n",
        "# Output\n",
        "print(\"Decoder Input Data:\")\n",
        "print(decoder_input_data)\n",
        "\n",
        "print(\"\\nDecoder Output Data (3D):\")\n",
        "print(decoder_output_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJ0ZRAO1g5ng",
        "outputId": "c6149eb8-11b6-495b-8195-71b33e5f66ed"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Input Data:\n",
            "[[ 3  0]\n",
            " [ 1  4]\n",
            " [ 2  6]\n",
            " [ 1  8]\n",
            " [ 2 10]\n",
            " [11  0]\n",
            " [12 13]]\n",
            "\n",
            "Decoder Output Data (3D):\n",
            "[[[ 0]\n",
            "  [ 0]]\n",
            "\n",
            " [[ 4]\n",
            "  [ 5]]\n",
            "\n",
            " [[ 6]\n",
            "  [ 7]]\n",
            "\n",
            " [[ 8]\n",
            "  [ 9]]\n",
            "\n",
            " [[10]\n",
            "  [ 0]]\n",
            "\n",
            " [[ 0]\n",
            "  [ 0]]\n",
            "\n",
            " [[13]\n",
            "  [ 0]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
        "\n",
        "# Example dataset (small English to French pairs)\n",
        "data = [(\"hello\", \"bonjour\"),\n",
        "        (\"how are you\", \"comment ça va\"),\n",
        "        (\"I am fine\", \"je vais bien\"),\n",
        "        (\"what is your name\", \"comment tu t'appelles\"),\n",
        "        (\"my name is\", \"je m'appelle\"),\n",
        "        (\"thank you\", \"merci\"),\n",
        "        (\"goodbye\", \"au revoir\")]\n",
        "\n",
        "# Separate into English and French sentences\n",
        "english_sentences = [pair[0] for pair in data]\n",
        "french_sentences = [pair[1] for pair in data]\n",
        "\n",
        "# Tokenize the English and French sentences\n",
        "english_tokenizer = Tokenizer()\n",
        "french_tokenizer = Tokenizer()\n",
        "\n",
        "english_tokenizer.fit_on_texts(english_sentences)\n",
        "french_tokenizer.fit_on_texts(french_sentences)\n",
        "\n",
        "# Convert text to sequences of integers\n",
        "english_sequences = english_tokenizer.texts_to_sequences(english_sentences)\n",
        "french_sequences = french_tokenizer.texts_to_sequences(french_sentences)\n",
        "\n",
        "# Pad the sequences to have uniform length\n",
        "max_english_seq_len = max([len(seq) for seq in english_sequences])\n",
        "max_french_seq_len = max([len(seq) for seq in french_sequences])\n",
        "\n",
        "english_padded = pad_sequences(english_sequences, maxlen=max_english_seq_len, padding='post')\n",
        "french_padded = pad_sequences(french_sequences, maxlen=max_french_seq_len, padding='post')\n",
        "\n",
        "# Prepare decoder input and output data\n",
        "decoder_input_data = french_padded[:, :-1]\n",
        "decoder_output_data = french_padded[:, 1:]\n",
        "\n",
        "# Expand decoder output to 3D shape for sparse_categorical_crossentropy\n",
        "decoder_output_data = np.expand_dims(decoder_output_data, -1)\n",
        "\n",
        "# Define the parameters for the model\n",
        "embedding_dim = 128\n",
        "units = 256\n",
        "\n",
        "# Define the encoder\n",
        "encoder_inputs = Input(shape=(None,))  # English input shape\n",
        "encoder_embedding = Embedding(input_dim=len(english_tokenizer.word_index) + 1, output_dim=embedding_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(units, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Define the decoder\n",
        "decoder_inputs = Input(shape=(None,))  # French input shape\n",
        "decoder_embedding = Embedding(input_dim=len(french_tokenizer.word_index) + 1, output_dim=embedding_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(french_tokenizer.word_index) + 1, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [english_padded, decoder_input_data],\n",
        "    decoder_output_data,\n",
        "    batch_size=2,  # You can adjust the batch size\n",
        "    epochs=10,    # Number of epochs for training\n",
        "    validation_split=0.2,\n",
        "    verbose=1      # Verbosity level\n",
        ")\n",
        "\n",
        "# Output the final metrics\n",
        "print(\"Final Training Loss:\", history.history['loss'][-1])\n",
        "print(\"Final Training Accuracy:\", history.history['accuracy'][-1])\n",
        "print(\"Final Validation Loss:\", history.history['val_loss'][-1])\n",
        "print(\"Final Validation Accuracy:\", history.history['val_accuracy'][-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VnLmlLPg73W",
        "outputId": "eed49f5d-e26a-4bd7-e35d-c975fd1050ba"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 239ms/step - accuracy: 0.2250 - loss: 2.6338 - val_accuracy: 0.7500 - val_loss: 2.5605\n",
            "Epoch 2/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3063 - loss: 2.5804 - val_accuracy: 0.7500 - val_loss: 2.4993\n",
            "Epoch 3/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1813 - loss: 2.5365 - val_accuracy: 0.7500 - val_loss: 2.4235\n",
            "Epoch 4/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.2125 - loss: 2.4515 - val_accuracy: 0.7500 - val_loss: 2.2687\n",
            "Epoch 5/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.3688 - loss: 2.2649 - val_accuracy: 0.7500 - val_loss: 1.9867\n",
            "Epoch 6/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3063 - loss: 2.1087 - val_accuracy: 0.7500 - val_loss: 1.6227\n",
            "Epoch 7/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3688 - loss: 1.7252 - val_accuracy: 0.7500 - val_loss: 1.4503\n",
            "Epoch 8/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.2438 - loss: 1.8825 - val_accuracy: 0.7500 - val_loss: 1.6479\n",
            "Epoch 9/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.2438 - loss: 1.7889 - val_accuracy: 0.7500 - val_loss: 1.8289\n",
            "Epoch 10/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2937 - loss: 1.6359 - val_accuracy: 0.7500 - val_loss: 1.9564\n",
            "Final Training Loss: 1.5041592121124268\n",
            "Final Training Accuracy: 0.4000000059604645\n",
            "Final Validation Loss: 1.956399917602539\n",
            "Final Validation Accuracy: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to decode sequence (translation)\n",
        "def decode_sequence(input_seq):\n",
        "    # Create separate encoder model\n",
        "    encoder_model = Model(encoder_inputs, encoder_states)  # Define encoder_model using the encoder inputs and states from your training model\n",
        "\n",
        "    # Encode the input as state vectors\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate an empty target sequence of length 1, initialized with the start token index\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    # Instead of '<start>', use the actual index if available, otherwise use 0\n",
        "    target_seq[0, 0] = french_tokenizer.word_index.get('comment', 0)\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        # Instead of creating a separate decoder model with get_output_at,\n",
        "        # Directly use the decoder_lstm and decoder_dense layers\n",
        "        output_tokens, h, c = decoder_lstm(decoder_embedding(target_seq), initial_state=states_value)\n",
        "        output_tokens = decoder_dense(output_tokens)\n",
        "\n",
        "        # Get the predicted token index\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = french_tokenizer.index_word.get(sampled_token_index, '')  # Safe access\n",
        "\n",
        "        # Check if the sampled word is defined\n",
        "        if sampled_word:  # Only add if the word is valid\n",
        "            decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "        # Check for stopping condition\n",
        "        if sampled_word == 'au' or len(decoded_sentence.split()) > max_french_seq_len:  # Use a word from your vocabulary to stop, like 'au' in 'au revoir'\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1)\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()  # Remove leading space"
      ],
      "metadata": {
        "id": "dDc_91YIhFu6"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}